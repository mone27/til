{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Coding with LLM and long output\n",
    "date: 2025-02-16\n",
    "author: Simone Massaro\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my development of the `polars-qt` package I have to work with a pretty long file that [defines all units](https://github.com/mone27/polars-qt/blob/main/src/units/definitions.rs). I changed the API of my library to I needed to do a bulk update of my files (i.e. add square brakets around all dimensions so that `length` becomes `[length]`) which is a pretty tedious task to do manually and using an LLM is perfect fit. The LLM can reliably do the replacement but the problem is that the response output length is not enough to give me the entire file as output, but the input is long enough to handle the entire file. I am using github copilot with [ChatGPT-4o](https://platform.openai.com/docs/models/gpt-4o), which has a return context of 16k tokens and an input context of 128k tokens. This means that I need to split my file in at least 3 chunks.\n",
    "\n",
    "Initially I was copy pasting each chunk into the LLM, getting the output and then repeating. However I realized that if I can first load the entire file and then ask the LLM to give the me the output between function `foo` and `bar` and this worked pretty reliably so I only need to copy paste the chunks one. To be honest this insights doesn't really change my productivity that much but is an interesting fact about LLMs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
